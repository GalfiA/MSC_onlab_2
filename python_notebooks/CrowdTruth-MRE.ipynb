{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:60% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%pylab inline\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cause = pd.read_csv(r'E:\\Egyetem\\tmp\\Medical-Relation-Extraction\\ground_truth_cause.csv')\n",
    "treat = pd.read_csv(r'E:\\Egyetem\\tmp\\Medical-Relation-Extraction\\ground_truth_treat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cause.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dk = cause.groupby(\"expert\")\n",
    "dk.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "nan = float('nan')\n",
    "cause[cause.expert == cause.expert]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cause[cause.expert == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(cause)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels(df):\n",
    "    expert = df.expert\n",
    "    crowd = df.crowd\n",
    "    label = 0\n",
    "    if expert == 1:\n",
    "        label = 1\n",
    "    elif pd.isnull(expert) and crowd > 0:\n",
    "        label = 1\n",
    "    \n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#expert = cause.expert.tolist()\n",
    "#crowd = cause.crowd.tolist()\n",
    "\n",
    "#train_label = np.where(expert == 0 or (pd.isnull(expert) and crowd > 0),1,0)\n",
    "\n",
    "train_label_cause = []\n",
    "train_label_treat = []\n",
    "\n",
    "for index, row in cause.iterrows():\n",
    "    tmp_label = extract_labels(row)\n",
    "    train_label_cause.append(tmp_label)\n",
    "    \n",
    "for index, row in treat.iterrows():\n",
    "    tmp_label = extract_labels(row)\n",
    "    train_label_treat.append(tmp_label)\n",
    "    \n",
    "len(train_label_treat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cause_plain = pd.read_excel(r'E:\\Egyetem\\tmp\\Medical-Relation-Extraction\\train_dev_test\\ground_truth_cause.xlsx')\n",
    "test_treat_plain = pd.read_excel(r'E:\\Egyetem\\tmp\\Medical-Relation-Extraction\\train_dev_test\\ground_truth_treat.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_cause_plain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cause_plain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cause = cause.term1_UMLS + ' ' + cause.UMLS_seed_relation + ' ' + cause.term2_UMLS\n",
    "train_treat = treat.term1_UMLS + ' ' + treat.UMLS_seed_relation + ' ' + treat.term2_UMLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(test_cause_plain.term1_UMLS[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_cause_plain = test_cause_plain.astype(str)\n",
    "test_treat_plain = test_treat_plain.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cause = test_cause_plain.term1_UMLS + ' ' + test_cause_plain.UMLS_seed_relation + ' ' + test_cause_plain.term2_UMLS\n",
    "test_treat = test_treat_plain.term1_UMLS + ' ' + test_treat_plain.UMLS_seed_relation + ' ' + test_treat_plain.term2_UMLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_cause.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cause.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label_cause = []\n",
    "test_label_treat = []\n",
    "\n",
    "for index, row in test_cause_plain.iterrows():\n",
    "    tmp_label = extract_labels(row)\n",
    "    test_label_cause.append(tmp_label)\n",
    "    \n",
    "for index, row in test_treat_plain.iterrows():\n",
    "    tmp_label = extract_labels(row)\n",
    "    test_label_treat.append(tmp_label)\n",
    "    \n",
    "len(test_label_cause)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In the next part, I will use only the *cause* table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So far I have the following:\n",
    "\n",
    "- *train_cause* : the sentences I use for training the model\n",
    "- *train_label_cause* : the matching labels to the training data\n",
    "- *test_cause* : this contains the testing data\n",
    "- *test_label_cause* : the matching labels to the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "lr  = LogisticRegression(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit(train_cause)\n",
    "tr_vectors = X.transform(train_cause)\n",
    "tst_vectors = X.transform(test_cause)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(tr_vectors,train_label_cause)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pred = lr.predict(tst_vectors)\n",
    "print(\"Logistic Regression Test accuracy : {}\".format(accuracy_score(test_label_cause, lr_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now trying to built a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(X.vocabulary_)\n",
    "NUM_LABELS = 2\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data, val_data = split(train_cause, test_size=0.3, random_state=SEED)\n",
    "tr_labels, val_labels = split(train_label_cause, test_size=0.3, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data_vecs = torch.FloatTensor(X.transform(tr_data).toarray())\n",
    "val_data_vecs = torch.FloatTensor(X.transform(val_data).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data_loader = [(sample, label) for sample, label in zip(tr_data_vecs, tr_labels)]\n",
    "val_data_loader = [(sample, label) for sample, label in zip(val_data_vecs, val_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_iterator = DataLoader(tr_data_loader,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=True,\n",
    "                            )\n",
    "\n",
    "valid_iterator = DataLoader(val_data_loader,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=False,\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MRE(nn.Module):\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MRE, self).__init__()\n",
    "    \n",
    "        self.hidden1 = nn.Linear(n_inputs, 10)\n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.hidden2 = nn.Linear(10, 8)\n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "        self.hidden3 = nn.Linear(8, 1)\n",
    "        self.act3 = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, bow_vec):\n",
    "        bow_vec = self.hidden1(bow_vec)\n",
    "        bow_vec = self.act1(bow_vec)\n",
    "        \n",
    "        bow_vec = self.hidden2(bow_vec)\n",
    "        bow_vec = self.act2(bow_vec)\n",
    "        \n",
    "        bow_vec = self.hidden3(bow_vec)\n",
    "        bow_vec = self.act3(bow_vec)\n",
    "        return F.log_softmax(bow_vec, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MRE(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def class_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch\n",
    "    \"\"\"\n",
    "    rounded_preds = preds.argmax(1)\n",
    "    correct = (rounded_preds == y).float()\n",
    "    \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for texts, labels in iterator:\n",
    "        texts = texts.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                  \n",
    "        predictions = model(texts)\n",
    "\n",
    "        loss = criterion(predictions, labels)\n",
    "        acc = class_accuracy(predictions, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for texts, labels in iterator:\n",
    "            \n",
    "            texts = texts.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            predictions = model(texts)\n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            acc = class_accuracy(predictions, labels)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## N_EPOCHS = 50\n",
    "\n",
    "## best_valid_loss = float('inf')\n",
    "\n",
    "## for epoch in range(N_EPOCHS):\n",
    "\n",
    "    ## start_time = time.time()\n",
    "    \n",
    "    ## train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    \n",
    "    ## valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    ## end_time = time.time()\n",
    "\n",
    "    ## epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    ## print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    ## print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    ## print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = tr_vectors.shape[1]\n",
    "input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import History \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend import clear_session\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kerasmodel = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(256, input_dim=input_dim, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(64, activation=\"softmax\"),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "kerasmodel.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_array = np.array(train_label_cause)\n",
    "label_array_test = np.array(test_label_cause)\n",
    "history = History()\n",
    "\n",
    "history = kerasmodel.fit(tr_vectors, label_array, epochs=10, batch_size=64, callbacks=[history],\n",
    "                         validation_data=(tst_vectors, label_array_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kerasmodel.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = kerasmodel.evaluate(tr_vectors, label_array, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "val_loss, val_accuracy = kerasmodel.evaluate(tst_vectors, label_array_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(val_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
