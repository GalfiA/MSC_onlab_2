{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previous version: CrowdTruth-MRE\n",
    "This notebook was created because this way the project stays cleaner and more understandeable.\n",
    "\n",
    "This time I try to organize the notebook more and label everything.\n",
    "The content of this notebook overlaps with the previous one, at least at the beginning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One : Imports\n",
    "In this part there are the imports of the necessary libraries and data files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%pylab inline\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cause_raw = pd.ExcelFile(r'E:\\Egyetem\\tmp\\Medical-Relation-Extraction\\train_dev_test\\ground_truth_cause.xlsx')\n",
    "treat_raw = pd.ExcelFile(r'E:\\Egyetem\\tmp\\Medical-Relation-Extraction\\train_dev_test\\ground_truth_treat.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### These are the train datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##cause_train = pd.read_csv(r'E:\\Egyetem\\tmp\\Medical-Relation-Extraction\\ground_truth_cause.csv')\n",
    "##treat_train = pd.read_csv(r'E:\\Egyetem\\tmp\\Medical-Relation-Extraction\\ground_truth_treat.csv')\n",
    "\n",
    "cause_train = pd.read_excel(cause_raw, 'train')\n",
    "treat_train = pd.read_excel(treat_raw, 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### These are the test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##cause_test = pd.read_excel(r'E:\\Egyetem\\tmp\\Medical-Relation-Extraction\\train_dev_test\\ground_truth_cause.xlsx')\n",
    "##treat_test = pd.read_excel(r'E:\\Egyetem\\tmp\\Medical-Relation-Extraction\\train_dev_test\\ground_truth_treat.xlsx')\n",
    "\n",
    "cause_test = pd.read_excel(cause_raw, 'test')\n",
    "treat_test = pd.read_excel(treat_raw, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Two : Preparing the data\n",
    "Below here happens the preparation of the data for the NLP model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label extraction function:\n",
    "This function creats a label to every row in the datasets:\n",
    "- this label is **1**:\n",
    "    - if the experts' score is 1\n",
    "    - there is no expert score but the crowd score is greater than 0\n",
    "- every other case the label is **0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels(df):\n",
    "    expert = df.expert\n",
    "    crowd = df.crowd\n",
    "    label = 0\n",
    "    if expert == 1:\n",
    "        label = 1\n",
    "    elif pd.isnull(expert) and crowd > 0:\n",
    "        label = 1\n",
    "    \n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence modification function:\n",
    "This function modifies the medical sentence:\n",
    "- switch both *term1* and *term2* in the sencence with the word **ENTITY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mod_sentence(df):\n",
    "    sentence = df.sentence\n",
    "    term1 = df.term1\n",
    "    term2 = df.term2\n",
    "    return_sentence_part1 = sentence.replace(term1, \"ENTITY1\")\n",
    "    return_sentence_part2 = return_sentence_part1.replace(term2, \"ENTITY2\")\n",
    "    \n",
    "    return return_sentence_part2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the learning data to the model\n",
    "Now it is time to use the functions on the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First let's use the *cause* table to create the *sentence-label* pairs for the model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cause_train_sentence = []\n",
    "for index, row in cause_train.iterrows():\n",
    "    tmp_sentence = mod_sentence(row)\n",
    "    cause_train_sentence.append(tmp_sentence)\n",
    "\n",
    "cause_train_label = []\n",
    "for index, row in cause_train.iterrows():\n",
    "    tmp_label = extract_labels(row)\n",
    "    cause_train_label.append(tmp_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then create the same pairs for the testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cause_test_sentence = []\n",
    "for index, row in cause_test.iterrows():\n",
    "    tmp_sentence = mod_sentence(row)\n",
    "    cause_test_sentence.append(tmp_sentence)\n",
    "\n",
    "cause_test_label = []\n",
    "for index, row in cause_test.iterrows():\n",
    "    tmp_label = extract_labels(row)\n",
    "    cause_test_label.append(tmp_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's do the same to the *treat* table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treat_train_sentence = []\n",
    "for index, row in treat_train.iterrows():\n",
    "    tmp_sentence = mod_sentence(row)\n",
    "    treat_train_sentence.append(tmp_sentence)\n",
    "\n",
    "treat_train_label = []\n",
    "for index, row in treat_train.iterrows():\n",
    "    tmp_label = extract_labels(row)\n",
    "    treat_train_label.append(tmp_label)\n",
    "    \n",
    "treat_test_sentence = []\n",
    "for index, row in treat_test.iterrows():\n",
    "    tmp_sentence = mod_sentence(row)\n",
    "    treat_test_sentence.append(tmp_sentence)\n",
    "\n",
    "treat_test_label = []\n",
    "for index, row in treat_test.iterrows():\n",
    "    tmp_label = extract_labels(row)\n",
    "    treat_test_label.append(tmp_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cause_train_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now the data is prepared:\n",
    "- **cause_train_sentence** / **treat_train_sentence** : these are conitaining the sentences to train the model\n",
    "- **cause_train_label** / **treat_train_label** : these conitain the labels for the training data\n",
    "- **cause_test_sentence** / **treat_test_sentence** : these are conitaining the sentences for testing\n",
    "- **cause_test_label** / **treat_test_label** : these conitain the labels for the testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Three : Building a model\n",
    "Now the data is prepared, it's time for building a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first try I will create a **Logistic Regression**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_c  = LogisticRegression(n_jobs=-1)\n",
    "\n",
    "cause_vectorizer = CountVectorizer()\n",
    "X_c = cause_vectorizer.fit(cause_train_sentence)\n",
    "tr_vectors_c = X_c.transform(cause_train_sentence)\n",
    "tst_vectors_c = X_c.transform(cause_test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_c.fit(tr_vectors_c,cause_train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_pred_c = lr_c.predict(tst_vectors_c)\n",
    "print(\"Logistic Regression Test accuracy : {}\".format(accuracy_score(cause_test_label, lr_pred_c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_t = LogisticRegression(n_jobs=-1)\n",
    "\n",
    "treat_vectorizer = CountVectorizer()\n",
    "X_t = treat_vectorizer.fit(treat_train_sentence)\n",
    "tr_vectors_t = X_t.transform(treat_train_sentence)\n",
    "tst_vectors_t = X_t.transform(treat_test_sentence)\n",
    "\n",
    "lr_t.fit(tr_vectors_t,treat_train_label)\n",
    "\n",
    "lr_pred_t = lr_t.predict(tst_vectors_t)\n",
    "print(\"Logistic Regression Test accuracy : {}\".format(accuracy_score(treat_test_label, lr_pred_t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression results:\n",
    "- **CAUSE**:\n",
    "    - Accuracy: 95.5%\n",
    "- **TREAT**:\n",
    "    - Accuracy: 76%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this simple solution gives pretty fair results, I am going to try to improve that with a **neural network**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "word_to_ix = vectorizer.fit(treat_train_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word_to_ix.vocabulary_)\n",
    "OUT_DIM = 2\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data, val_data = treat_train_sentence, treat_test_sentence\n",
    "tr_labels, val_labels = treat_train_label, treat_test_label\n",
    "\n",
    "tr_data_vecs = torch.FloatTensor(word_to_ix.transform(tr_data).toarray())\n",
    "val_data_vecs = torch.FloatTensor(word_to_ix.transform(val_data).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data_loader = [(sample, label) for sample, label in zip(tr_data_vecs, tr_labels)]\n",
    "val_data_loader = [(sample, label) for sample, label in zip(val_data_vecs, val_labels)]\n",
    "\n",
    "train_iterator = DataLoader(tr_data_loader,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=True,\n",
    "                            )\n",
    "\n",
    "valid_iterator = DataLoader(val_data_loader,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=False,\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRE(nn.Module):\n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        super(MRE, self).__init__()\n",
    "    \n",
    "        self.hidden1 = nn.Linear(vocab_size, 250)\n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.hidden2 = nn.Linear(250, 100)\n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "        self.hidden3 = nn.Linear(100, num_labels)\n",
    "        self.act3 = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, bow_vec):\n",
    "        bow_vec = self.hidden1(bow_vec)\n",
    "        bow_vec = self.act1(bow_vec)\n",
    "        \n",
    "        bow_vec = self.hidden2(bow_vec)\n",
    "        bow_vec = self.act2(bow_vec)\n",
    "        \n",
    "        bow_vec = self.hidden3(bow_vec)\n",
    "        bow_vec = self.act3(bow_vec)\n",
    "        return F.log_softmax(bow_vec, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MRE(OUT_DIM, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch\n",
    "    \"\"\"\n",
    "    rounded_preds = preds.argmax(1)\n",
    "    correct = (rounded_preds == y).float()\n",
    "    \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for texts, labels in iterator:\n",
    "        texts = texts.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                  \n",
    "        predictions = model(texts)\n",
    "\n",
    "        loss = criterion(predictions, labels)\n",
    "        acc = class_accuracy(predictions, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for texts, labels in iterator:\n",
    "            \n",
    "            texts = texts.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            predictions = model(texts)\n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            acc = class_accuracy(predictions, labels)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 15\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    \n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results:\n",
    "- **CAUSE**:\n",
    "    - Not yet calculated\n",
    "- **TREAT**:\n",
    "    - *Train*:\n",
    "        - Accuracy: 99.55%\n",
    "        - Loss: 0.318\n",
    "    - *Validation*:\n",
    "        - Accuracy: 79.38%\n",
    "        - Loss: 0.515"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now the next step is an LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This CountVectorizer will:\n",
    "- have max 5000 featuers\n",
    "- use the words' lexical form\n",
    "- ignore stop words ('/n' etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer_embed = CountVectorizer(max_features=5000, tokenizer=LemmaTokenizer(), stop_words=\"english\")\n",
    "\n",
    "word_to_ix_embed = vectorizer_embed.fit(cause_train_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = word_to_ix_embed.build_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer(\"i have had the best time playing tennis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_to_ix_embed.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to tokens function:\n",
    "This function wil convert the sentences to an array of numbers:\n",
    "- if the word is in the vocabulary (defined above), then the word gets the value set in the vocabulary\n",
    "- if the word is not in the vocabulary then it gets _unknown_ value (this case the value 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token(sentences, analyzer, vocab):\n",
    "    sentences_as_tokens = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokens = analyzer(sentence)\n",
    "        \n",
    "        words_to_tokens = []\n",
    "        \n",
    "        for token in tokens:\n",
    "            if token in vocab:\n",
    "                words_to_tokens.append(vocab[token])\n",
    "            else:\n",
    "                words_to_tokens.append(3000)\n",
    "        \n",
    "        if not words_to_tokens:\n",
    "            words_to_tokens.append(3000)\n",
    "        \n",
    "        sentences_as_tokens.append(torch.LongTensor(words_to_tokens).to(device))\n",
    "    \n",
    "    return sentences_as_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = word_to_ix_embed.vocabulary_\n",
    "cause_as_ids = text_to_token(cause_train_sentence, analyzer, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cause_train_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cause_as_ids[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same length sentences\n",
    "Because the sentences should be the same length I have to fill out the shorter ones.\n",
    "The _padding_value_ will be 5001 because 0-4999 is the value for different words, 5000 is the value for undefined words and the next value is 5001.\n",
    "This value shows that the sentence is ended and it is just a placeholder value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cause_padded = pad_sequence(cause_as_ids, batch_first=True, padding_value=5001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cause_train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloaders_wtih_padding(train_data, train_label, val_data, val_label, word_to_ix):\n",
    "    train_tokens = text_to_token(train_data, analyzer, word_to_ix.vocabulary_)\n",
    "    tr_vectors = pad_sequence(train_tokens, batch_first=True, padding_value=5001)\n",
    "    \n",
    "    tr_labels = torch.LongTensor(train_label).to(device)\n",
    "    tr_lens = torch.LongTensor(\n",
    "        [len(i) for i in text_to_token(train_data, analyzer, word_to_ix.vocabulary_)]\n",
    "    )\n",
    "    \n",
    "    tr_sents = train_data\n",
    "    \n",
    "    \n",
    "    val_tokens = text_to_token(val_data, analyzer, word_to_ix.vocabulary_)\n",
    "    val_vectors = pad_sequence(val_tokens, batch_first=True, padding_value=5001)\n",
    "    \n",
    "    val_labels = torch.LongTensor(val_label).to(device)\n",
    "    val_lens = torch.LongTensor(\n",
    "        [len(i) for i in text_to_token(val_data, analyzer, word_to_ix.vocabulary_)]\n",
    "    )\n",
    "    \n",
    "    val_sents = val_data\n",
    "    \n",
    "    \n",
    "    tr_data_loader = [\n",
    "        (sample, label, length, sent)\n",
    "        for sample, label, length, sent in zip(\n",
    "            tr_vectors, tr_labels, tr_lens, tr_sents\n",
    "        )\n",
    "    ]\n",
    "    val_data_loader = [\n",
    "        (sample, label, length, sent)\n",
    "        for sample, label, length, sent in zip(\n",
    "            val_vectors, val_labels, val_lens, val_sents\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return tr_data_loader, val_data_loader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data_loader, val_data_loader = dataloaders_wtih_padding(\n",
    "    cause_train_sentence, cause_train_label,\n",
    "    cause_test_sentence, cause_test_label,\n",
    "    word_to_ix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, num_labels, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=5001)\n",
    "        self.embedding.weight.requires_grad = True\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, num_layers=1, bidirectional=False)\n",
    "        self.linear = nn.Linear(hidden_dim, num_labels)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "    \n",
    "    def forward(self, text, sequence_lens):\n",
    "        embedded = self.embedding(text)\n",
    "    \n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, sequence_lens, enforce_sorted=False, batch_first=True)\n",
    "        packed_outputs, (h, c) = self.lstm(packed)\n",
    "        \n",
    "        y = self.linear(h[-1])\n",
    "        log_probs = F.log_softmax(y, dim=1)\n",
    "        return log_probs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word_to_ix_embed.vocabulary_)\n",
    "INPUT_DIM = VOCAB_SIZE + 2\n",
    "OUTPUT_DIM = 2\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 20\n",
    "criterion = nn.NLLLoss()\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "model = LSTMClassifier(OUTPUT_DIM, INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = DataLoader(\n",
    "        tr_data_loader,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "valid_iterator = DataLoader(\n",
    "        val_data_loader,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_embed(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_prec = 0\n",
    "    epoch_recall = 0\n",
    "    epoch_fscore = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        text_vecs = batch[0]\n",
    "        labels = batch[1]\n",
    "        sen_lens = batch[2]\n",
    "        texts = batch[3]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions = model(text_vecs, sen_lens)\n",
    "        \n",
    "        loss = criterion(predictions, labels)\n",
    "\n",
    "        prec, recall, fscore = calculate_performance(predictions, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_prec += prec.item()\n",
    "        epoch_recall += recall.item()\n",
    "        epoch_fscore += fscore.item()\n",
    "    \n",
    "    return(epoch_loss / len(iterator),\n",
    "           epoch_prec / len(iterator),\n",
    "           epoch_recall / len(iterator),\n",
    "           epoch_fscore / len(iterator)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_embed(model, iterator, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_prec = 0\n",
    "    epoch_recall = 0\n",
    "    epoch_fscore = 0\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for batch in iterator:\n",
    "            text_vecs = batch[0]\n",
    "            labels = batch[1]\n",
    "            sen_lens = batch[2]\n",
    "            texts = batch[3]\n",
    "            \n",
    "            \n",
    "        predictions = model(text_vecs, sen_lens)\n",
    "        \n",
    "        loss = criterion(predictions, labels)\n",
    "\n",
    "        prec, recall, fscore = calculate_performance(predictions, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_prec += prec.item()\n",
    "        epoch_recall += recall.item()\n",
    "        epoch_fscore += fscore.item()\n",
    "    \n",
    "    return(epoch_loss / len(iterator),\n",
    "           epoch_prec / len(iterator),\n",
    "           epoch_recall / len(iterator),\n",
    "           epoch_fscore / len(iterator)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## N_EPOCHS = 15\n",
    "\n",
    "## best_valid_loss = float(\"inf\")\n",
    "\n",
    "## for epoch in range(N_EPOCHS):\n",
    "    ## start_time = time.time()\n",
    "    \n",
    "    ## train_loss, train_prec, train_rec, train_fscore = train_embed(model, train_iterator, optimizer, criterion)\n",
    "    \n",
    "    ## valid_loss, valid_prec, valid_rec, valid_fscore = evaluate_embed(model, valid_iterator, criterion)\n",
    "    \n",
    "    ## end_time = time.time()\n",
    "\n",
    "    ## epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    ## print(f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
    "    ## print(f\"\\tTrain Loss: {train_loss:.3f} | Train Prec: {train_prec*100:.2f}% | Train Rec: {train_rec*100:.2f}% | Train Fscore: {train_fscore*100:.2f}%\")\n",
    "    ## print(f\"\\t Val. Loss: {valid_loss:.3f} |  Val Prec: {valid_prec*100:.2f}% | Val Rec: {valid_rec*100:.2f}% | Val Fscore: {valid_fscore*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
